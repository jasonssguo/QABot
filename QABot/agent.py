# agent.py
from utils import *
from config import *
from prompt import *
import urllib.parse

import os
from langchain.chains import LLMChain, LLMRequestsChain
from langchain.prompts import PromptTemplate
from langchain.vectorstores.chroma import Chroma
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.agents import ZeroShotAgent, AgentExecutor, Tool
from langchain.memory import ConversationBufferMemory
from langchain.output_parsers import ResponseSchema, StructuredOutputParser


class Agent():
    def __init__(self):
        # å®šä¹‰å‘é‡åº“æŒä¹…åŒ–ç›®å½•
        self.db_path = os.path.join(os.path.dirname(__file__), './data/db/')
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = get_embeddings_model()
        # åˆå§‹åŒ–Chromaå‘é‡åº“
        self.vdb = self.init_chroma_db()

    def init_chroma_db(self):
        """
        åˆå§‹åŒ–Chromaå‘é‡åº“ï¼šå¦‚æœåº“ä¸­æ— æ•°æ®ï¼Œåˆ™æ·»åŠ ç¤ºä¾‹æ•°æ®ï¼›å¦åˆ™ç›´æ¥åŠ è½½
        """
        # æ£€æŸ¥ChromaæŒä¹…åŒ–ç›®å½•æ˜¯å¦å­˜åœ¨ï¼ˆåˆ¤æ–­æ˜¯å¦å·²æœ‰æ•°æ®ï¼‰
        if os.path.exists(self.db_path) and len(os.listdir(self.db_path)) > 0:
            # åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“
            vdb = Chroma(
                persist_directory=self.db_path,
                embedding_function=self.embeddings
            )
            print(f"æˆåŠŸåŠ è½½å·²æœ‰å‘é‡åº“ï¼Œå½“å‰åº“ä¸­æ–‡æ¡£æ•°é‡ï¼š{vdb._collection.count()}")
        else:
            # æ— æ•°æ®æ—¶ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£å¹¶å†™å…¥å‘é‡åº“
            # è¿™é‡Œæ›¿æ¢æˆä½ è‡ªå·±çš„ä¸šåŠ¡æ–‡æ¡£
            sample_documents = [
                Document(
                    page_content="å¯»åŒ»é—®è¯ç½‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å¥åº·ä¿¡æ¯æœåŠ¡å¹³å°ï¼Œæä¾›ç–¾ç—…å’¨è¯¢ã€è¯å“æŸ¥è¯¢ã€åŒ»ç”Ÿåœ¨çº¿é—®è¯Šç­‰æœåŠ¡ã€‚",
                    metadata={"source": "å¯»åŒ»é—®è¯ç½‘å®˜ç½‘"}
                ),
                Document(
                    page_content="å¯»åŒ»é—®è¯ç½‘çš„å®¢æœç”µè¯æ˜¯400-123-4567ï¼Œå·¥ä½œæ—¶é—´ä¸ºæ¯å¤©9:00-18:00ã€‚",
                    metadata={"source": "å¯»åŒ»é—®è¯ç½‘å¸®åŠ©ä¸­å¿ƒ"}
                )
            ]
            # å°†æ–‡æ¡£å†™å…¥Chromaå‘é‡åº“
            vdb = Chroma.from_documents(
                documents=sample_documents,
                embedding=self.embeddings,
                persist_directory=self.db_path
            )
            # æŒä¹…åŒ–æ•°æ®ï¼ˆå…³é”®æ­¥éª¤ï¼Œç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜ï¼‰
            vdb.persist()
            print(f"å·²åˆ›å»ºæ–°å‘é‡åº“å¹¶å†™å…¥{len(sample_documents)}æ¡ç¤ºä¾‹æ–‡æ¡£")
        return vdb

    def generic_func(self, x, query):
        # print(f'query{query}')
        prompt = PromptTemplate.from_template(GENERIC_PROMPT_TPL)
        llm_chain = LLMChain(
            llm=get_llm_model(),
            prompt=prompt,
            verbose=os.getenv("VERBOSE")
        )
        return llm_chain.run(query)

    def retrival_func(self, x, query):
        # å¬å›å¹¶è¿‡æ»¤æ–‡æ¡£ï¼ˆk=5è¡¨ç¤ºè¿”å›æœ€ç›¸ä¼¼çš„5æ¡ï¼‰
        documents = self.vdb.similarity_search_with_relevance_scores(query, k=5)
        # æ‰“å°æŸ¥è¯¢ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•ï¼ˆæ›¿æ¢åŸæ¥çš„print+exitï¼‰
        # print(f"\nåŸå§‹æŸ¥è¯¢ç»“æœï¼ˆæ–‡æ¡£+ç›¸ä¼¼åº¦å¾—åˆ†ï¼‰ï¼š{documents}")

        # è¿‡æ»¤ç›¸ä¼¼åº¦å¾—åˆ†>0.7çš„æ–‡æ¡£
        query_result = [doc[0].page_content for doc in documents if doc[1] > 0.5]
        # print(f"\nè¿‡æ»¤åçš„æœ‰æ•ˆæ–‡æ¡£ï¼š{query_result}")

        # å¡«å……ç­”æ¡ˆ
        prompt = PromptTemplate.from_template(RETRIVAL_PROMPT_TPL)
        retrival_chain = LLMChain(
            llm=get_llm_model(),
            prompt=prompt,
            verbose=os.getenv('VERBOSE')
        )
        inputs = {
            'query': query,
            'query_result': '\n\n'.join(query_result) if len(query_result) else 'æ²¡æœ‰æŸ¥åˆ°'
        }
        return retrival_chain.run(inputs)

    def graph_func(self, x, query):
        # å‘½åå®ä½“è¯†åˆ«
        response_schemas = [
            ResponseSchema(type='list', name='disease', description='ç–¾ç—…åç§°å®ä½“'),
            ResponseSchema(type='list', name='symptom', description='ç–¾ç—…ç—‡çŠ¶å®ä½“'),
            ResponseSchema(type='list', name='drug', description='è¯å“åç§°å®ä½“'),
        ]
        output_parser = StructuredOutputParser(response_schemas=response_schemas)
        format_instructions = structured_output_parser(response_schemas)

        ner_prompt = PromptTemplate(
            template=NER_PROMPT_TPL,
            partial_variables={'format_instructions': format_instructions},
            input_variables=['query']
        )

        ner_chain = LLMChain(
            llm=get_llm_model(),
            prompt=ner_prompt,
            verbose=os.getenv('VERBOSE')
        )

        result = ner_chain.run({
            'query': query
        })

        ner_result = output_parser.parse(result)
        # print(ner_result)
        # exit()


        # å‘½åå®ä½“è¯†åˆ«ç»“æœï¼Œå¡«å……æ¨¡æ¿
        graph_templates = []
        for key, template in GRAPH_TEMPLATE.items():
            slot = template['slots'][0]
            slot_values = ner_result[slot]
            # print(slot_values)
            # exit()
            for value in slot_values:
                graph_templates.append({
                    'question': replace_token_in_string(template['question'], {slot: value}),
                    'cypher': replace_token_in_string(template['cypher'], {slot: value}),
                    'answer': replace_token_in_string(template['answer'], {slot: value}),
                })
            # print(graph_templates)
            # exit()
        if not graph_templates:
            return

        # è®¡ç®—é—®é¢˜ç›¸ä¼¼åº¦ï¼Œç­›é€‰æœ€ç›¸å…³é—®é¢˜
        graph_documents = [
            Document(page_content=template['question'], metadata=template)
            for template in graph_templates
        ]
        # print(graph_documents)
        # exit()
        db = FAISS.from_documents(graph_documents, get_embeddings_model())
        graph_documents_filter = db.similarity_search_with_relevance_scores(query, k=5)
        # print(graph_documents_filter)

        # æ‰§è¡ŒCQLï¼Œæ‹¿åˆ°ç»“æœ
        query_result = []
        neo4j_conn = get_neo4j_conn()
        # result = neo4j_conn.run("CALL dbms.components() YIELD versions, name RETURN name, versions[0] as version").data()
        # if result:
        #     print(f"âœ… Neo4jè¿æ¥æˆåŠŸï¼ˆpy2neoï¼‰ï¼")
        #     print(f"ğŸ“Œ æ•°æ®åº“ä¿¡æ¯ï¼š{result[0]['name']} {result[0]['version']}")
        # print(neo4j_conn)
        for document in graph_documents_filter:
            question = document[0].page_content
            cypher = document[0].metadata['cypher']
            answer = document[0].metadata['answer']
            # print(cypher)
            try:
                result = neo4j_conn.run(cypher).data()
                if result and any(value for value in result[0].values()):
                    answer_str = replace_token_in_string(answer, list(result[0].items()))
                    # print(answer_str)
                    # exit()
                    query_result.append(f'é—®é¢˜: {question}\nç­”æ¡ˆ: {answer_str}')
            except:
                pass
        print(query_result)
        # exit()

        # æ€»ç»“ç­”æ¡ˆ
        prompt = PromptTemplate.from_template(GRAPH_PROMPT_TPL)
        graph_chain = LLMChain(
            llm=get_llm_model(),
            prompt=prompt,
            verbose=os.getenv('VERBOSE')
        )
        inputs = {
            'query': query,
            'query_result': "\n\n".join(query_result) if len(query_result) else "æ²¡æœ‰æŸ¥åˆ°"
        }
        return graph_chain.run(inputs)

    def search_func(self, query):
        # // ç½‘ç»œæœç´¢çš„æ¨¡å—
        prompt = PromptTemplate.from_template(SEARCH_PROMPT_TPL)
        llm_chain = LLMChain(
            llm=get_llm_model(),
            prompt=prompt,
            verbose=os.getenv('VERBOSE')
        )
        llm_request_chain = LLMRequestsChain(
            llm_chain=llm_chain,
            requests_key='query_result'
        )
        encoded_query = urllib.parse.quote(query)
        baidu_url = f'https://www.baidu.com/s?wd={encoded_query}&rn=10'
        c360_url = f'https://www.so.com/s?q='+query.replace(' ','+')
        inputs = {
            'query': query,
            # 'url': 'https://www.google.com/search?q=' + query.replace(' ', '+')
            'url': c360_url
        }
        return llm_request_chain.run(inputs)


    def parse_tools(self, tools, query):
        prompt = PromptTemplate.from_template(PARSE_TOOLS_PROMPT_TPL)
        llm_chain = LLMChain(
            llm=get_llm_model(),
            prompt=prompt,
            verbose=os.getenv('VERBOSE')
        )
        # æ‹¼æ¥å·¥å…·æè¿°å‚æ•°
        tools_description = ''
        for tool in tools:
            tools_description += tool.name + ':' + tool.description + '\n'
        # print(tools_description)
        # exit()
        result = llm_chain.invoke({'tools_description': tools_description, 'query': query})
        # print(result)
        # exit()
        # è§£æå·¥å…·å‡½æ•°
        for tool in tools:
            if tool.name == result['text']:
                return tool
        return tools[0]

    def query(self, query):
        tools = [
            Tool.from_function(
                name="generic_func",
                func=lambda x: self.generic_func(x, query),
                description="å¯ä»¥è§£ç­”é€šç”¨é¢†åŸŸçš„çŸ¥è¯†ï¼Œä¾‹å¦‚æ‰“æ‹›å‘¼ã€é—®ä½ æ˜¯è°ç­‰é—®é¢˜"
            ),
            Tool.from_function(
                name="retrival_func",
                func=lambda x: self.retrival_func(x, query),
                description="ç”¨äºå›ç­”å¯»åŒ»é—®è¯ç½‘ç›¸å…³é—®é¢˜"
            ),
            Tool(
                name="graph_func",
                func=lambda x: self.graph_func(x, query),
                description="ç”¨äºå›ç­”ç–¾ç—…ã€ç—‡çŠ¶ã€è¯ç‰©ç­‰åŒ»ç–—ç›¸å…³é—®é¢˜"
            ),
            Tool(
                name="search_func",
                func=self.search_func,
                description="å…¶ä»–å·¥å…·æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œé€šè¿‡æœç´¢å¼•æ“å›ç­”é€šç”¨ç±»é—®é¢˜"
            )
        ]
        # tool = self.parse_tools(tools,query=query)
        # return tool.func(query)

        prefix = '''è¯·ç”¨ä¸­æ–‡ï¼Œå°½ä½ æ‰€èƒ½å›ç­”ä»¥ä¸‹çš„é—®é¢˜ã€‚

é‡è¦è§„åˆ™ï¼š
1. å½“ä½ æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ "Final Answer: [ä½ çš„ç­”æ¡ˆ]" æ ¼å¼ç›´æ¥å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚
2. åªæœ‰å½“ä½ éœ€è¦ä½¿ç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯æ—¶ï¼Œæ‰ä½¿ç”¨ "Action: [å·¥å…·åç§°]" æ ¼å¼ã€‚
3. ä¸è¦é‡å¤è°ƒç”¨åŒä¸€ä¸ªå·¥å…·ã€‚
4. å¯¹äºç®€å•çš„é—®å€™ï¼Œä½¿ç”¨ generic_func å·¥å…·è·å–ç­”æ¡ˆåï¼Œç›´æ¥ç»™å‡º Final Answerã€‚

å›ç­”æ ¼å¼ï¼š
- å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼šThought: [æ€è€ƒ] Action: [å·¥å…·å] Action Input: [è¾“å…¥]
- å¦‚æœå¯ä»¥ç›´æ¥å›ç­”ï¼šThought: [æ€è€ƒ] Final Answer: [ç­”æ¡ˆ]

æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹çš„å·¥å…·ï¼š'''
        suffix = """Begin!

Question: {input}
Thought: {agent_scratchpad}"""

        agent_prompt = ZeroShotAgent.create_prompt(
            tools=tools,
            prefix=prefix,
            suffix=suffix,
            input_variables=['input', 'agent_scratchpad', 'chat_history']
        )

        llm_chain = LLMChain(llm=get_llm_model(), prompt=agent_prompt)
        agent = ZeroShotAgent(llm_chain=llm_chain)
        memory = ConversationBufferMemory(memory_key='chat_history')

        agent_chain = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=tools,
            memory=memory,
            handle_parsing_errors=True,
            max_iterations=3,  # é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            verbose=os.getenv('VERBOSE')
        )
        return agent_chain.run({'input':query})

if __name__ == '__main__':
    agent = Agent()
    print("\n===== æŸ¥è¯¢ç»“æœ =====")
    # print(agent.retrival_func('ä»‹ç»ä¸€ä¸‹å¯»åŒ»é—®è¯ç½‘æ˜¯ä»€ä¹ˆ'))

    print(agent.query('å¯»åŒ»é—®è¯ç½‘çš„å®¢æœç”µè¯æ˜¯å¤šå°‘ï¼Ÿ'))
    # print(agent.generic_func('ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ'))

    # print(agent.graph_func('åŒ–è„“æ€§é¼»çª¦ç‚æ˜¯é¼»ç‚çš„å¹¶å‘ç—‡å—ï¼Ÿ'))
    # print(agent.graph_func('æ„Ÿå†’ä¸€èˆ¬æ˜¯ç”±ä»€ä¹ˆå¼•èµ·çš„ï¼Ÿ'))
    # agent.graph_func('æ„Ÿå†’åƒä»€ä¹ˆè¯å¥½å¾—å¿«ï¼Ÿå¯ä»¥åƒé˜¿è«è¥¿æ—å—ï¼Ÿ')
    # è°ƒç”¨æµ‹è¯•
    # print(agent.search_func('é™ˆåç¼–ç¨‹éƒ½æœ‰ä»€ä¹ˆè¯¾ç¨‹'))
    # print(agent.query('ä½ å¥½'))
    # exit()
    # print(agent.query('å¯»åŒ»é—®è¯ç½‘è·å¾—è¿‡å“ªäº›æŠ•èµ„'))
    # print(agent.query('é¼»ç‚å’Œæ„Ÿå†’æ˜¯å¹¶å‘ç—‡å—ï¼Ÿ'))
    # print(agent.query('é¼»ç‚æ€ä¹ˆæ²»ç–—ï¼Ÿ'))
    # print(agent.query('çƒ§æ©™å­å¯ä»¥æ²»æ„Ÿå†’å—ï¼Ÿ'))